# Procon/src/train.py (LSTMå¯¾å¿œç‰ˆ)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split
from pathlib import Path

# --- ãƒ‘ã‚¹è¨­å®š ---
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent
data_path = project_root / 'data' / 'lstm_data.npy'
labels_path = project_root / 'data' / 'lstm_labels.npy'
# ä¿å­˜å…ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãªãã€ãƒ•ã‚©ãƒ«ãƒ€åã«å¤‰æ›´
model_save_path = project_root / 'data' / 'conducting_model_savedmodel'

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    # 1. .npyãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {data_path}")
    X = np.load(data_path)
    y = np.load(labels_path)
    
    # ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã¨ã‚¯ãƒ©ã‚¹æ•°ã‚’å–å¾—
    num_samples, timesteps, num_features = X.shape
    num_classes = len(np.unique(y))
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ã€‚å½¢çŠ¶: ({num_samples}, {timesteps}, {num_features}), ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")

    # 2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 3. LSTMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    model = Sequential([
        # å…¥åŠ›å±¤: (ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°, 1ãƒ•ãƒ¬ãƒ¼ãƒ ã‚ãŸã‚Šã®ç‰¹å¾´é‡æ•°)
        LSTM(64, return_sequences=True, input_shape=(timesteps, num_features)),
        BatchNormalization(),
        Dropout(0.5),
        
        LSTM(64),
        BatchNormalization(),
        Dropout(0.5),
        
        Dense(64, activation='relu'),
        # å‡ºåŠ›å±¤: ã‚¯ãƒ©ã‚¹æ•°ã«åˆã‚ã›ã‚‹
        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    # 4. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    print("\nLSTMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    # (EarlyStoppingã¯éå­¦ç¿’ã‚’é˜²ãã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™)
    callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]
    history = model.fit(
        X_train, y_train,
        epochs=50,  # ã‚¨ãƒãƒƒã‚¯æ•°ã¯èª¿æ•´ãŒå¿…è¦
        batch_size=32,
        validation_split=0.2,
        callbacks=callbacks
    )

    # 5. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print("-" * 30)
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ­£è§£ç‡ (Accuracy): {accuracy:.4f}")

    # 6. ãƒ¢ãƒ‡ãƒ«ã®ã€Œé‡ã¿ã€ã®ã¿ã‚’ä¿å­˜
    weights_save_path = project_root / 'data' / 'conducting_model.weights.h5'
    model.save_weights(weights_save_path)
    print(f"ğŸ‰ å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼: {weights_save_path}")

if __name__ == '__main__':
    main()